{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of candidate suspect list\n",
    "\n",
    "- Julia M. Gauglitz\n",
    "- Wout Bittremieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as ssignal\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.neighbors.kde import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='{asctime} [{levelname}/{processName}] {message}',\n",
    "                    style='{', level=logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "logger = logging.getLogger('suspect_list')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_shift_annotations = pd.read_csv(\n",
    "    'https://docs.google.com/spreadsheets/d/'\n",
    "    '1-xh2XpSqdsa4yU-ATpDRxmpZEH6ht982jCCATFOpkyM/'\n",
    "    'export?format=csv&gid=566878567')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(ids: pd.DataFrame, max_ppm: float = 20,\n",
    "               min_shared_peaks: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter high-quality identifications according to the given maximum ppm\n",
    "    deviation and minimum number of shared peaks. Identifications without an\n",
    "    InChI will be omitted as well.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The tabular identifications retrieved from GNPS.\n",
    "    max_ppm : float\n",
    "        The maximum ppm deviation.\n",
    "    min_shared_peaks : int\n",
    "        The minimum number of shared peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The identifications retained after filtering.\n",
    "    \"\"\"\n",
    "    return (ids[(ids['MZErrorPPM'].abs() <= max_ppm) &\n",
    "                (ids['SharedPeaks'] >= min_shared_peaks)]\n",
    "            .dropna(subset=['INCHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs: pd.DataFrame, min_cosine: float = 0.8) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only consider pairs with a cosine similarity that exceeds the given\n",
    "    cosine threshold.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs : pd.DataFrame\n",
    "        The tabular pairs retrieved from GNPS.\n",
    "    min_cosine : float\n",
    "        The minimum cosine used to retain high-quality pairs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The pairs filtered by minimum cosine similarity.\n",
    "    \"\"\"\n",
    "    return pairs[pairs['Cosine'] >= min_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_summary(summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cluster select as representative the scan with the highest\n",
    "    precursor intensity.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    summary : pd.DataFrame\n",
    "        The tabular summary retrieved from GNPS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The summary with only the scans with the highest precursor intensity\n",
    "        for each cluster.\n",
    "    \"\"\"\n",
    "    summary = (summary.reindex(summary.groupby(['dataset', 'cluster index'])\n",
    "                               ['sum(precursor intensity)'].idxmax())\n",
    "               .dropna().reset_index(drop=True)\n",
    "               [['dataset', 'cluster index', 'parent mass', 'ScanNumber',\n",
    "                 'Original_Path']])\n",
    "    summary['cluster index'] = summary['cluster index'].astype(int)\n",
    "    summary['ScanNumber'] = summary['ScanNumber'].astype(int)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suspects(ids: pd.DataFrame, pairs: pd.DataFrame,\n",
    "                      summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate suspects from identifications and aligned spectra pairs.\n",
    "    Provenance about the spectra pairs is added from the summary.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The filtered identifications.\n",
    "    pairs : pd.DataFrame\n",
    "        The filtered pairs.\n",
    "    summary : pd.DataFrame\n",
    "        The filtered summary information for the clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with information about both spectra forming a suspect\n",
    "        identification.\n",
    "    \"\"\"\n",
    "    # Form suspects of library and unidentified spectra pairs.\n",
    "    suspects = pd.concat([\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID1'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID1'])\n",
    "        .rename(columns={'CLUSTERID2': 'SuspectIndex'}),\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID2'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID2'])\n",
    "        .rename(columns={'CLUSTERID1': 'SuspectIndex'})],\n",
    "        ignore_index=True, sort=False).dropna(axis=1)\n",
    "    \n",
    "    # TODO: Properly handle this warning.\n",
    "    if not suspects['SuspectIndex'].is_unique:\n",
    "        logger.warning('Multiple analog matches per suspect scan found')\n",
    "    \n",
    "    # Add provenance information for the library and suspect scans.\n",
    "    suspects = (suspects[['dataset', 'INCHI', 'Compound_Name', 'Adduct',\n",
    "                          'Cosine', 'Precursor_MZ', 'SpectrumID', '#Scan#',\n",
    "                          'SuspectIndex']]\n",
    "                .rename(columns={'Compound_Name': 'CompoundName',\n",
    "                                 'Precursor_MZ': 'LibraryPrecursorMZ',\n",
    "                                 'SpectrumID': 'LibraryID',\n",
    "                                 '#Scan#': 'ClusterScanNr'}))\n",
    "    suspects = (pd.merge(suspects, summary,\n",
    "                         left_on=['dataset', 'SuspectIndex'],\n",
    "                         right_on=['dataset', 'cluster index'])\n",
    "                .drop(columns=['SuspectIndex', 'cluster index'])\n",
    "                .rename(columns={'parent mass': 'SuspectPrecursorMZ',\n",
    "                                 'Original_Path': 'SuspectPath',\n",
    "                                 'ScanNumber': 'SuspectScanNr'}))\n",
    "    return suspects.drop(columns=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_filter_putative_mass_shifts(\n",
    "        suspects: pd.DataFrame, mass_shift_annotations: pd.DataFrame,\n",
    "        min_delta_mz: float = 1, interval_width: float = 1.0,\n",
    "        bin_width: float = 0.02, prominence: float = 0.5) -> pd.DataFrame:\n",
    "    suspects['DeltaMZ'] = \\\n",
    "        suspects['LibraryPrecursorMZ'] - suspects['SuspectPrecursorMZ']\n",
    "    # Remove suspects without a mass shift.\n",
    "    suspects = suspects[suspects['DeltaMZ'].abs() > min_delta_mz].copy()\n",
    "    # Assign putative identifications to the mass shifts.\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bin_width)\n",
    "    for mz in np.arange(math.floor(suspects['DeltaMZ'].min()),\n",
    "                        math.ceil(suspects['DeltaMZ'].max())):\n",
    "        mask = suspects.index[suspects['DeltaMZ'].between(\n",
    "            mz - interval_width / 2, mz + interval_width / 2)]\n",
    "        if len(mask) == 0:\n",
    "            continue\n",
    "        bins = (np.linspace(mz - interval_width / 2,\n",
    "                            mz + interval_width / 2,\n",
    "                            int(interval_width / bin_width) + 1)\n",
    "                + bin_width / 2)\n",
    "        kde.fit(suspects.loc[mask, 'DeltaMZ'].values.reshape(-1, 1))\n",
    "        density = np.exp(kde.score_samples(bins.reshape(-1, 1)))\n",
    "        peaks_i, prominences = ssignal.find_peaks(density,\n",
    "                                                  prominence=prominence)\n",
    "        order = np.argsort(prominences['prominences'])[::-1]\n",
    "        for delta_mz, start_i, stop_i in zip(\n",
    "                bins[peaks_i[order]],\n",
    "                prominences['left_bases'][order],\n",
    "                prominences['right_bases'][order]):\n",
    "            putative_id = mass_shift_annotations[\n",
    "                (mass_shift_annotations['mz delta'].abs()\n",
    "                 - abs(delta_mz)).abs() < bin_width]\n",
    "            mask_putative_id = suspects.loc[mask].index[\n",
    "                suspects.loc[mask, 'DeltaMZ'].between(\n",
    "                    bins[start_i], bins[stop_i])]\n",
    "            suspects.loc[mask_putative_id, 'GroupDeltaMZ'] = delta_mz\n",
    "            suspects.loc[mask_putative_id, 'AtomicDifference'] = \\\n",
    "                '|'.join(putative_id['atomic difference'])\n",
    "            suspects.loc[mask_putative_id, 'Rationale'] = \\\n",
    "                '|'.join(putative_id['rationale'].fillna('unknown'))\n",
    "    \n",
    "    # Only use the top suspect (by cosine score) per combination of library\n",
    "    # spectra and putative identification.\n",
    "    suspects = (suspects.sort_values(['Cosine'], ascending=False)\n",
    "                .drop_duplicates(['CompoundName', 'GroupDeltaMZ']))\n",
    "    \n",
    "    return (suspects.sort_values(['CompoundName', 'GroupDeltaMZ'])\n",
    "            .reset_index(drop=True)\n",
    "            [['INCHI', 'CompoundName', 'Adduct', 'DeltaMZ', 'GroupDeltaMZ',\n",
    "              'AtomicDifference', 'Rationale', 'Cosine',\n",
    "              'LibraryPrecursorMZ', 'LibraryID', 'ClusterScanNr',\n",
    "              'SuspectPrecursorMZ', 'SuspectScanNr', 'SuspectPath']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate suspect library matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria to form a suspect:\n",
    "\n",
    "- Identification ≤ 20 ppm.\n",
    "- Identification ≥ 6 shared peaks.\n",
    "- Identification has to include InChI.\n",
    "- Cosine ≥ 0.8.\n",
    "- The spectrum with maximal intensity is chosen as cluster representative.\n",
    "\n",
    "Criteria for putative suspect explanations:\n",
    "\n",
    "- Corrected delta _m_/_z_'s for common shifts are determined using kernel density estimation with bandwith 0.02 within each 1 _m_/_z_ window (centered around unit _m_/_z_'s).\n",
    "- The corrected delta _m_/_z_ is compared to known mass shifts with a 0.02 _m_/_z_ threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_prefix = 'ftp://massive.ucsd.edu/MSV000084314/other'\n",
    "ids, pairs, summary = [], [], []\n",
    "logger.info('Retrieve cluster information')\n",
    "msv_ids = pd.read_csv('datasets.csv', header=None).squeeze()\n",
    "msv_ids_retry = []\n",
    "while len(msv_ids) > 0:\n",
    "    for msv_id in tqdm.tqdm(msv_ids, desc='Datasets read', unit='dataset'):\n",
    "        try:\n",
    "            ids.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/IDENTIFICATIONS/{msv_id}_identifications.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'Compound_Name', 'Adduct', 'Precursor_MZ', 'INCHI',\n",
    "                    'SpectrumID', 'LibraryQualityString', '#Scan#',\n",
    "                    'MZErrorPPM', 'SharedPeaks']))\n",
    "            ids[-1]['dataset'] = msv_id\n",
    "            pairs.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/PAIRS/{msv_id}_pairs.tsv', sep='\\t',\n",
    "                usecols=['CLUSTERID1', 'CLUSTERID2', 'Cosine']))\n",
    "            pairs[-1]['dataset'] = msv_id\n",
    "            summary.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/CLUSTERSUMMARY/{msv_id}_summary.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'cluster index', 'sum(precursor intensity)',\n",
    "                    'parent mass', 'Original_Path', 'ScanNumber']))\n",
    "            summary[-1]['dataset'] = msv_id\n",
    "        except ValueError:\n",
    "            logger.warning(\"Couldn't process dataset %s\", msv_id)\n",
    "        except IOError:\n",
    "            msv_ids_retry.append(msv_id)\n",
    "    msv_ids = msv_ids_retry\n",
    "    msv_ids_retry = []\n",
    "    \n",
    "logger.info('Compile suspect pairs')\n",
    "ids = filter_ids(pd.concat(ids, ignore_index=True))\n",
    "pairs = filter_pairs(pd.concat(pairs, ignore_index=True))\n",
    "summary = filter_summary(pd.concat(summary, ignore_index=True))\n",
    "suspects = generate_suspects(ids, pairs, summary)\n",
    "logger.info('Assign putative explanations to mass shifts')\n",
    "suspects = assign_filter_putative_mass_shifts(suspects,\n",
    "                                              mass_shift_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore delta masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mzs = (suspects['GroupDeltaMZ'].value_counts().reset_index()\n",
    "             .rename(columns={'index': 'DeltaMZ', 'GroupDeltaMZ': 'count'})\n",
    "            .sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width * 1.5, height / 1.5))\n",
    "\n",
    "ax.bar(delta_mzs['DeltaMZ'], delta_mzs['count'], width=0.4, color='black')\n",
    "\n",
    "ax.set_xlim(-300, 300)\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel('Delta m/z')\n",
    "ax.set_ylabel(f'Number of suspects')\n",
    "\n",
    "plt.savefig('delta_mz.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mz_count_cdf = np.cumsum(delta_mzs['count']) / delta_mzs['count'].sum()\n",
    "delta_mz_count_cdf = np.insert(delta_mz_count_cdf.values, 0, 0)\n",
    "\n",
    "prop_threshold = 0.8\n",
    "rank_threshold = np.argmax(delta_mz_count_cdf > prop_threshold)\n",
    "count_threshold = delta_mzs['count'].iloc[rank_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "ax.plot(np.arange(len(delta_mz_count_cdf)), delta_mz_count_cdf)\n",
    "\n",
    "ax.plot([rank_threshold, rank_threshold], [0, prop_threshold], 'k--')\n",
    "ax.plot([0, rank_threshold], [prop_threshold, prop_threshold], 'k--')\n",
    "\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "\n",
    "ax.set_xlim(0, ax.get_xlim()[1])\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel('Delta m/z frequency rank')\n",
    "ax.set_ylabel('Proportion of suspects')\n",
    "\n",
    "plt.savefig('delta_mz_count_cdf.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Frequency threshold at {prop_threshold:.0%} CDF = {count_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export frequent/infrequent suspect libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_freq = suspects[suspects['GroupDeltaMZ'].isin(\n",
    "    delta_mzs[delta_mzs['count'] >= count_threshold]['DeltaMZ'])]\n",
    "suspects_infreq = suspects[~suspects['GroupDeltaMZ'].isin(\n",
    "    delta_mzs[delta_mzs['count'] >= count_threshold]['DeltaMZ'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of frequent suspects: {len(suspects_freq)} '\n",
    "      f'({suspects_freq[\"GroupDeltaMZ\"].nunique()} unique m/z deltas)')\n",
    "print(f'Number of infrequent suspects: {len(suspects_infreq)} '\n",
    "      f'({suspects_infreq[\"GroupDeltaMZ\"].nunique()} unique m/z deltas)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_freq.to_csv('suspect_library_freq.csv', index=False)\n",
    "suspects_infreq.to_csv('suspect_library_infreq.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
