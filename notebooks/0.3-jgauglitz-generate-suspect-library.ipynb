{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of candidate suspect list\n",
    "\n",
    "- Julia M. Gauglitz\n",
    "- Wout Bittremieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import functools\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as ssignal\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='{asctime} [{levelname}/{processName}] {message}',\n",
    "                    style='{', level=logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "logger = logging.getLogger('suspect_list')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_shift_annotations = pd.read_csv(\n",
    "    'https://docs.google.com/spreadsheets/d/'\n",
    "    '1-xh2XpSqdsa4yU-ATpDRxmpZEH6ht982jCCATFOpkyM/'\n",
    "    'export?format=csv&gid=566878567')\n",
    "mass_shift_annotations['mz delta'] = (mass_shift_annotations['mz delta']\n",
    "                                      .astype(np.float64))\n",
    "mass_shift_annotations['priority'] = (mass_shift_annotations['priority']\n",
    "                                      .astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(ids: pd.DataFrame, max_ppm: float = 20,\n",
    "               min_shared_peaks: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter high-quality identifications according to the given maximum ppm\n",
    "    deviation and minimum number of shared peaks. Identifications without an\n",
    "    InChI will be omitted as well.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The tabular identifications retrieved from GNPS.\n",
    "    max_ppm : float\n",
    "        The maximum ppm deviation.\n",
    "    min_shared_peaks : int\n",
    "        The minimum number of shared peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The identifications retained after filtering.\n",
    "    \"\"\"\n",
    "    return (ids[(ids['MZErrorPPM'].abs() <= max_ppm) &\n",
    "                (ids['SharedPeaks'] >= min_shared_peaks)]\n",
    "            .dropna(subset=['INCHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs: pd.DataFrame, min_cosine: float = 0.8) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only consider pairs with a cosine similarity that exceeds the given\n",
    "    cosine threshold.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs : pd.DataFrame\n",
    "        The tabular pairs retrieved from GNPS.\n",
    "    min_cosine : float\n",
    "        The minimum cosine used to retain high-quality pairs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The pairs filtered by minimum cosine similarity.\n",
    "    \"\"\"\n",
    "    return pairs[pairs['Cosine'] >= min_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(cluster_info: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cluster select as representative the scan with the highest\n",
    "    precursor intensity.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    cluster_info : pd.DataFrame\n",
    "        The tabular cluster info retrieved from GNPS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Clusters without duplicated spectra by keeping only the scan with the\n",
    "        highest precursor intensity for each cluster.\n",
    "    \"\"\"\n",
    "    cluster_info = (\n",
    "        cluster_info.reindex(cluster_info.groupby(\n",
    "            ['dataset', 'cluster index'])['sum(precursor intensity)'].idxmax())\n",
    "        .dropna().reset_index(drop=True)\n",
    "        [['dataset', 'cluster index', 'parent mass', 'ScanNumber',\n",
    "          'Original_Path']])\n",
    "    cluster_info['cluster index'] = cluster_info['cluster index'].astype(int)\n",
    "    cluster_info['ScanNumber'] = cluster_info['ScanNumber'].astype(int)\n",
    "    return cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suspects(ids: pd.DataFrame, pairs: pd.DataFrame,\n",
    "                      summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate suspects from identifications and aligned spectra pairs.\n",
    "    Provenance about the spectra pairs is added from the summary.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The filtered identifications.\n",
    "    pairs : pd.DataFrame\n",
    "        The filtered pairs.\n",
    "    summary : pd.DataFrame\n",
    "        The filtered summary information for the clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with information about both spectra forming a suspect\n",
    "        identification.\n",
    "    \"\"\"\n",
    "    # Form suspects of library and unidentified spectra pairs.\n",
    "    suspects = pd.concat([\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID1'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID1'])\n",
    "        .rename(columns={'CLUSTERID2': 'SuspectIndex'}),\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID2'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID2'])\n",
    "        .rename(columns={'CLUSTERID1': 'SuspectIndex'})],\n",
    "        ignore_index=True, sort=False).dropna(axis=1)\n",
    "    \n",
    "    # TODO: Properly handle this warning.\n",
    "    if not suspects['SuspectIndex'].is_unique:\n",
    "        logger.warning('Multiple analog matches per suspect scan found')\n",
    "    \n",
    "    # Add provenance information for the library and suspect scans.\n",
    "    suspects = (suspects[['dataset', 'INCHI', 'Compound_Name', 'Adduct',\n",
    "                          'Cosine', 'Precursor_MZ', 'SpectrumID', '#Scan#',\n",
    "                          'SuspectIndex']]\n",
    "                .rename(columns={'Compound_Name': 'CompoundName',\n",
    "                                 'Precursor_MZ': 'LibraryPrecursorMZ',\n",
    "                                 'SpectrumID': 'LibraryID',\n",
    "                                 '#Scan#': 'ClusterScanNr'}))\n",
    "    suspects = (pd.merge(suspects, summary,\n",
    "                         left_on=['dataset', 'SuspectIndex'],\n",
    "                         right_on=['dataset', 'cluster index'])\n",
    "                .drop(columns=['SuspectIndex', 'cluster index'])\n",
    "                .rename(columns={'parent mass': 'SuspectPrecursorMZ',\n",
    "                                 'Original_Path': 'SuspectPath',\n",
    "                                 'ScanNumber': 'SuspectScanNr'}))\n",
    "    return suspects.drop(columns=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_filter_putative_mass_shifts(\n",
    "        suspects: pd.DataFrame, mass_shift_annotations: pd.DataFrame,\n",
    "        min_delta_mz: float = 1, interval_width: float = 1.0,\n",
    "        bin_width: float = 0.02, prominence: float = 0.5) -> pd.DataFrame:\n",
    "    suspects['DeltaMZ'] = \\\n",
    "        suspects['LibraryPrecursorMZ'] - suspects['SuspectPrecursorMZ']\n",
    "    # Remove suspects without a mass shift.\n",
    "    suspects = suspects[suspects['DeltaMZ'].abs() > min_delta_mz].copy()\n",
    "    # Assign putative identifications to the mass shifts.\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bin_width)\n",
    "    for mz in np.arange(math.floor(suspects['DeltaMZ'].min()),\n",
    "                        math.ceil(suspects['DeltaMZ'].max())):\n",
    "        mask = suspects.index[suspects['DeltaMZ'].between(\n",
    "            mz - interval_width / 2, mz + interval_width / 2)]\n",
    "        if len(mask) == 0:\n",
    "            continue\n",
    "        bins = (np.linspace(mz - interval_width / 2,\n",
    "                            mz + interval_width / 2,\n",
    "                            int(interval_width / bin_width) + 1)\n",
    "                + bin_width / 2)\n",
    "        kde.fit(suspects.loc[mask, 'DeltaMZ'].values.reshape(-1, 1))\n",
    "        density = np.exp(kde.score_samples(bins.reshape(-1, 1)))\n",
    "        peaks_i, prominences = ssignal.find_peaks(density,\n",
    "                                                  prominence=prominence)\n",
    "        order = np.argsort(prominences['prominences'])[::-1]\n",
    "        for delta_mz, start_i, stop_i in zip(\n",
    "                bins[peaks_i[order]],\n",
    "                prominences['left_bases'][order],\n",
    "                prominences['right_bases'][order]):\n",
    "            putative_id = mass_shift_annotations[\n",
    "                (mass_shift_annotations['mz delta'].abs()\n",
    "                 - abs(delta_mz)).abs() < bin_width]\n",
    "            mask_putative_id = suspects.loc[mask].index[\n",
    "                suspects.loc[mask, 'DeltaMZ'].between(\n",
    "                    bins[start_i], bins[stop_i])]\n",
    "            suspects.loc[mask_putative_id, 'GroupDeltaMZ'] = delta_mz\n",
    "            suspects.loc[mask_putative_id, 'AtomicDifference'] = \\\n",
    "                '|'.join(putative_id['atomic difference'].fillna('unknown'))\n",
    "            suspects.loc[mask_putative_id, 'Rationale'] = \\\n",
    "                '|'.join(putative_id['rationale'].fillna('unknown'))\n",
    "    \n",
    "    # Only use the top suspect (by cosine score) per combination of library\n",
    "    # spectra and putative identification.\n",
    "    suspects = (suspects.sort_values(['Cosine'], ascending=False)\n",
    "                .drop_duplicates(['CompoundName', 'GroupDeltaMZ']))\n",
    "    \n",
    "    return (suspects.sort_values(['CompoundName', 'GroupDeltaMZ'])\n",
    "            .reset_index(drop=True)\n",
    "            [['INCHI', 'CompoundName', 'Adduct', 'DeltaMZ', 'GroupDeltaMZ',\n",
    "              'AtomicDifference', 'Rationale', 'Cosine',\n",
    "              'LibraryPrecursorMZ', 'LibraryID', 'ClusterScanNr',\n",
    "              'SuspectPrecursorMZ', 'SuspectScanNr', 'SuspectPath']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate suspect library matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria to form a suspect:\n",
    "\n",
    "- Identification ≤ 20 ppm.\n",
    "- Identification ≥ 6 shared peaks.\n",
    "- Identification has to include InChI.\n",
    "- Cosine ≥ 0.8.\n",
    "- The spectrum with maximal precursor intensity is chosen as cluster representative.\n",
    "\n",
    "Criteria for putative suspect explanations:\n",
    "\n",
    "- Corrected delta _m_/_z_'s for common shifts are determined using kernel density estimation with bandwith 0.02 within each 1 _m_/_z_ window (centered around unit _m_/_z_'s).\n",
    "- The corrected delta _m_/_z_ is compared to known mass shifts with a 0.02 _m_/_z_ threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ppm = 20\n",
    "min_shared_peaks = 6\n",
    "min_cosine = 0.8\n",
    "min_delta_mz = 0.5\n",
    "interval_width = 1.0\n",
    "bin_width = 0.002\n",
    "prominence = 20\n",
    "max_dist = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'MSV000084314/updates/2020-10-08_mwang87_d7c866dd/other'\n",
    "ftp_prefix = f'ftp://massive.ucsd.edu/{base_url}'\n",
    "\n",
    "# Get the MassIVE IDs for all datasets processed in the living data analyses.\n",
    "ftp = ftplib.FTP('massive.ucsd.edu')\n",
    "ftp.login()\n",
    "ftp.cwd(f'{base_url}/CLUSTERINFO')\n",
    "msv_ids = [filename[:filename.find('_')] for filename in ftp.nlst()]\n",
    "\n",
    "# Generate the suspects.\n",
    "ids, pairs, clusters = [], [], []\n",
    "logger.info('Retrieve cluster information')\n",
    "for msv_id in tqdm.tqdm(msv_ids, desc='Datasets processed', unit='dataset'):\n",
    "    max_tries = 5\n",
    "    while max_tries > 0:\n",
    "        try:\n",
    "            ids.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/IDENTIFICATIONS/{msv_id}_identifications.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'Compound_Name', 'Adduct', 'Precursor_MZ', 'INCHI',\n",
    "                    'SpectrumID', 'LibraryQualityString', '#Scan#',\n",
    "                    'MZErrorPPM', 'SharedPeaks']))\n",
    "            ids[-1]['dataset'] = msv_id\n",
    "            pairs.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/PAIRS/{msv_id}_pairs.tsv', sep='\\t',\n",
    "                usecols=['CLUSTERID1', 'CLUSTERID2', 'Cosine']))\n",
    "            pairs[-1]['dataset'] = msv_id\n",
    "            clusters.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/CLUSTERINFO/{msv_id}_clustering.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'cluster index', 'sum(precursor intensity)',\n",
    "                    'parent mass', 'Original_Path', 'ScanNumber']))\n",
    "            clusters[-1]['dataset'] = msv_id\n",
    "        except ValueError:\n",
    "            logger.warning(\"Couldn't process dataset %s\", msv_id)\n",
    "            max_tries = 0\n",
    "        except IOError:\n",
    "            max_tries -= 1\n",
    "        else:\n",
    "            max_tries = 0\n",
    "    \n",
    "logger.info('Compile suspect pairs')\n",
    "ids = filter_ids(pd.concat(ids, ignore_index=True), max_ppm, min_shared_peaks)\n",
    "pairs = filter_pairs(pd.concat(pairs, ignore_index=True), min_cosine)\n",
    "clusters = filter_clusters(pd.concat(clusters, ignore_index=True))\n",
    "suspects_unfiltered = generate_suspects(ids, pairs, clusters)\n",
    "logger.info('Assign putative explanations to mass shifts')\n",
    "suspects = assign_filter_putative_mass_shifts(\n",
    "    suspects_unfiltered, mass_shift_annotations, min_delta_mz, interval_width,\n",
    "    bin_width, prominence, max_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore delta masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mzs = (suspects['GroupDeltaMZ'].value_counts().reset_index()\n",
    "             .rename(columns={'index': 'DeltaMZ', 'GroupDeltaMZ': 'count'})\n",
    "            .sort_values('count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_width = 1\n",
    "bin_width = 0.02\n",
    "prominence = 0.5\n",
    "\n",
    "mz = int(delta_mzs['DeltaMZ'].round(0).iloc[0])\n",
    "mask = suspects.index[suspects['DeltaMZ'].between(\n",
    "    mz - interval_width / 2, mz + interval_width / 2)]\n",
    "bins = (np.linspace(mz - interval_width / 2, mz + interval_width / 2,\n",
    "                    int(interval_width / bin_width) + 1) + bin_width / 2)\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=bin_width)\n",
    "kde.fit(suspects.loc[mask, 'DeltaMZ'].values.reshape(-1, 1))\n",
    "density = np.exp(kde.score_samples(bins.reshape(-1, 1)))\n",
    "peaks_i, prominences = ssignal.find_peaks(density, prominence=prominence)\n",
    "\n",
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "sns.distplot(suspects.loc[mask, 'DeltaMZ'], bins=bins, kde=False,\n",
    "             hist_kws={'alpha': 0.25}, ax=ax)\n",
    "ax.plot(bins, density, c=sns.color_palette()[0])\n",
    "ax.scatter(bins[peaks_i], density[peaks_i], s=50, marker='D')\n",
    "\n",
    "ax.set_xlabel('$m$/$z$')\n",
    "ax.set_ylabel('Number of suspects')\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(f'density_{mz}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width * 1.5, height / 1.5))\n",
    "\n",
    "ax.bar(delta_mzs['DeltaMZ'], delta_mzs['count'], width=0.4, color='black')\n",
    "\n",
    "ax.set_xlim(-300, 300)\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel('Delta m/z')\n",
    "ax.set_ylabel(f'Number of suspects')\n",
    "\n",
    "plt.savefig('delta_mz.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mz_count_cdf = np.cumsum(delta_mzs['count']) / delta_mzs['count'].sum()\n",
    "delta_mz_count_cdf = np.insert(delta_mz_count_cdf.values, 0, 0)\n",
    "\n",
    "prop_threshold = 0.8\n",
    "rank_threshold = np.argmax(delta_mz_count_cdf > prop_threshold)\n",
    "count_threshold = delta_mzs['count'].iloc[rank_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width, height))\n",
    "\n",
    "ax.plot(np.arange(len(delta_mz_count_cdf)), delta_mz_count_cdf)\n",
    "\n",
    "ax.plot([rank_threshold, rank_threshold], [0, prop_threshold], 'k--')\n",
    "ax.plot([0, rank_threshold], [prop_threshold, prop_threshold], 'k--')\n",
    "\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "\n",
    "ax.set_xlim(0, ax.get_xlim()[1])\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel('Delta m/z frequency rank')\n",
    "ax.set_ylabel('Proportion of suspects')\n",
    "\n",
    "plt.savefig('delta_mz_count_cdf.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Frequency threshold at {prop_threshold:.0%} CDF = {count_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export frequent/infrequent suspect libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_freq = suspects[suspects['GroupDeltaMZ'].isin(\n",
    "    delta_mzs[delta_mzs['count'] >= count_threshold]['DeltaMZ'])]\n",
    "suspects_infreq = suspects[~suspects['GroupDeltaMZ'].isin(\n",
    "    delta_mzs[delta_mzs['count'] >= count_threshold]['DeltaMZ'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of frequent suspects: {len(suspects_freq)} '\n",
    "      f'({suspects_freq[\"GroupDeltaMZ\"].nunique()} unique m/z deltas)')\n",
    "print(f'Number of infrequent suspects: {len(suspects_infreq)} '\n",
    "      f'({suspects_infreq[\"GroupDeltaMZ\"].nunique()} unique m/z deltas)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_freq.to_csv('../../data/suspect_library_freq.csv', index=False)\n",
    "suspects_infreq.to_csv('../../data/suspect_library_infreq.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
