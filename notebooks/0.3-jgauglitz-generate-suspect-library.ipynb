{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of candidate suspect list\n",
    "\n",
    "Julia M. Gauglitz\n",
    "\n",
    "Date: 4/20/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function 'id_clean' used for editing IDs table\n",
    "\n",
    "Define library ID inclusion criteria: \n",
    "1. MZErrorPPM <= 20\n",
    "2. SharedPeaks >= 6\n",
    "3. Include only entries with an INCHI; INCHI is not equal to 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(ids: pd.DataFrame, max_ppm: float = 20,\n",
    "               min_shared_peaks: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter high-quality identifications according to the given maximum ppm\n",
    "    deviation and minimum number of shared peaks. Identifications without an\n",
    "    InChI will be omitted as well.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The tabular identifications retrieved from GNPS.\n",
    "    max_ppm : float\n",
    "        The maximum ppm deviation.\n",
    "    min_shared_peaks : int\n",
    "        The minimum number of shared peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The identifications retained after filtering.\n",
    "    \"\"\"\n",
    "    return (ids[(ids['MZErrorPPM'].abs() <= max_ppm) &\n",
    "                (ids['SharedPeaks'] >= min_shared_peaks)]\n",
    "            .dropna(subset=['INCHI']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing pairs table\n",
    "\n",
    "Define criteria for pairs dictionary to include as candidate analog annotations (didn't filter any in example MSV000078547)\n",
    "1. Cosine >= 0.8\n",
    "2. Define DeltaMZ to be taken into account (i.e. 14, 16, 28); m/z delta +/- 20ppm difference at 1500 m/z ; assign putative_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_mass_diffs = {14.0: 'CH2', 16.0: 'O', 28.0: 'C2H4'}\n",
    "\n",
    "\n",
    "def get_frequent_mass_diffs(\n",
    "        delta_mzs: pd.Series, round_digits: int = 1,\n",
    "    min_mass_diff: float = 1, min_count: int = 20,\n",
    "    known_mass_diffs : Dict[float, str] = known_mass_diffs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get frequently reported delta m/zs.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    delta_mzs : pd.Series\n",
    "        A Series of observed delta m/zs.\n",
    "    round_digits : int\n",
    "        The number of decimals to use for rounding the delta m/zs.\n",
    "    min_mass_diff : float\n",
    "        The minimum absolute delta m/z to consider (i.e. exclude 0 deltas).\n",
    "    min_count : int\n",
    "        The minimum number of times the delta m/zs need to occur to be\n",
    "        considered.\n",
    "    known_mass_diffs : Dict[float, str]\n",
    "        A dictionary with as keys mass differences and as values likely\n",
    "        explanations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with as columns the observed delta m/z, the number of\n",
    "        times this delta m/z occurs, and its likely explanation (or the delta\n",
    "        m/z repeated if not possible).\n",
    "    \"\"\"\n",
    "    mass_diffs = (delta_mzs\n",
    "                  .apply(functools.partial(round, ndigits=round_digits))\n",
    "                  .value_counts().reset_index()\n",
    "                  .rename(columns={'index': 'DeltaMZ', 'DeltaMZ': 'count'}))\n",
    "    # Exclude unshifted and infrequent mass differences.\n",
    "    mass_diffs = mass_diffs[(mass_diffs['DeltaMZ'].abs() > min_mass_diff) &\n",
    "                            (mass_diffs['count'] > min_count)]\n",
    "    # Explain known/unknown mass differences.\n",
    "    mass_diffs['PutativeID'] = (mass_diffs['DeltaMZ'].abs()\n",
    "                                .map(known_mass_diffs))\n",
    "    mass_diffs['PutativeID'] = mass_diffs['PutativeID'].fillna(\n",
    "        mass_diffs['DeltaMZ'])\n",
    "    return mass_diffs\n",
    "    \n",
    "\n",
    "def pairs_explain_mass_diff(pairs: pd.DataFrame, delta_mass: float = 0.05,\n",
    "                            min_cosine: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Match mass differences between cluster pairs to known elements.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs : pd.DataFrame\n",
    "        The tabular pairs retrieved from GNPS.\n",
    "    delta_mass : float\n",
    "        The delta mass (Da) used to matched pairs mass differences to known\n",
    "        elements.\n",
    "    min_cosine : float\n",
    "        The minimum cosine used to retain high-quality pairs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The pairs to which an element could be matched based on their mass\n",
    "        differences.\n",
    "    \"\"\"\n",
    "    # Omit pairs with a low cosine score.\n",
    "    pairs = pairs[pairs['Cosine'] >= min_cosine]\n",
    "    # Find frequently occuring mass differences.\n",
    "    mass_diffs = get_frequent_mass_diffs(pairs['DeltaMZ'])\n",
    "    # Match mass differences to putative identifications.\n",
    "    pairs['PutativeID'] = np.nan\n",
    "    for mass_shift, putative_id in zip(mass_diffs['DeltaMZ'],\n",
    "                                       mass_diffs['PutativeID']):\n",
    "        matched_mass_shift = pairs['DeltaMZ'].between(\n",
    "            mass_shift - delta_mass, mass_shift + delta_mass)\n",
    "        pairs.loc[matched_mass_shift, 'PutativeID'] = putative_id\n",
    "    return pairs.dropna(subset=['PutativeID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for editing summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_max_intensity_scan_per_cluster(summary: pd.DataFrame) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cluster select as representative the scan with the highest\n",
    "    precursor intensity.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    summary : pd.DataFrame\n",
    "        The tabular summary retrieved from GNPS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The summary with only the scans with the highest precursor intensity\n",
    "        for each cluster.\n",
    "    \"\"\"\n",
    "    summary = (summary.reindex(summary.groupby(['dataset', 'cluster index'])\n",
    "                               ['sum(precursor intensity)'].idxmax())\n",
    "               .dropna().reset_index(drop=True)\n",
    "               [['dataset', 'cluster index', 'ScanNumber', 'Original_Path']])\n",
    "    summary['cluster index'] = summary['cluster index'].astype(int)\n",
    "    summary['ScanNumber'] = summary['ScanNumber'].astype(int)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID clustered spectra to add to library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID clustered spectra to add to library\n",
    "\n",
    "Function: Find the pairs, based on filtered input files. Create a column that contains the opposite clusterid, which is the scan number needed to add a new suspect annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_suspect_pairs(ids: pd.DataFrame, pairs: pd.DataFrame,\n",
    "                          summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine suspects pairs with identification and library information.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The filtered identifications.\n",
    "    pairs : pd.DataFrame\n",
    "        The pairs with mass difference explanations.\n",
    "    summary : pd.DataFrame\n",
    "        The summary information for the clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with information about both spectra forming the suspect\n",
    "        identification.\n",
    "    \"\"\"\n",
    "    # Combine pairs with identifications to form suspects.\n",
    "    suspects_plus = (pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID1'],\n",
    "                              right_on=['dataset', '#Scan#'])\n",
    "                     .drop(columns=['CLUSTERID1'])\n",
    "                     .rename(columns={'CLUSTERID2': 'SuspectIndex'}))\n",
    "    suspects_minus = (pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID2'],\n",
    "                               right_on=['dataset', '#Scan#'])\n",
    "                      .drop(columns=['CLUSTERID2'])\n",
    "                      .rename(columns={'CLUSTERID1': 'SuspectIndex'}))\n",
    "    suspects = pd.concat([suspects_plus, suspects_minus],\n",
    "                         ignore_index=True, sort=False).dropna(axis=1)\n",
    "    suspects['Mod'] = pd.Series(['addition'] * len(suspects_plus) +\n",
    "                                ['loss'] * len(suspects_minus))\n",
    "    # Only take the top suspect by cosine similarity.\n",
    "    suspects = (suspects.sort_values(['Cosine'], ascending=False)\n",
    "                .drop_duplicates(['Compound_Name', 'Mod', 'PutativeID']))\n",
    "    # TODO: Properly handle this warning.\n",
    "    if not suspects['SuspectIndex'].is_unique:\n",
    "        print('Multiple analog matches per suspect scan found')\n",
    "    \n",
    "    # Add provenance information for the library and suspect scans.\n",
    "    suspects = (suspects[['dataset', 'Compound_Name', 'Mod', 'PutativeID',\n",
    "                          'DeltaMZ', 'SpectrumID', '#Scan#', 'SuspectIndex']]\n",
    "                .rename(columns={'Compound_Name': 'CompoundName',\n",
    "                                 'SpectrumID': 'LibraryID',\n",
    "                                 '#Scan#': 'ClusterScanNr'}))\n",
    "    suspects = (pd.merge(suspects, summary, left_on=['dataset', 'SuspectIndex'],\n",
    "                         right_on=['dataset', 'cluster index'])\n",
    "                .drop(columns=['SuspectIndex', 'cluster index'])\n",
    "                .rename(columns={'Original_Path': 'SuspectPath',\n",
    "                                 'ScanNumber': 'SuspectScanNr'}))\n",
    "    return suspects.drop(columns=['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate suspect library using functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_all = []\n",
    "ftp_prefix = 'ftp://massive.ucsd.edu/MSV000084314/other'\n",
    "ids, pairs, summary = [], [], []\n",
    "for msv_id in tqdm.tqdm(pd.read_csv('datasets.csv', header=None).squeeze()[:5],\n",
    "                        desc='Datasets read', unit='dataset'):\n",
    "    ids.append(pd.read_csv(\n",
    "        f'{ftp_prefix}/IDENTIFICATIONS/{msv_id}_identifications.tsv',\n",
    "        sep='\\t', usecols=['Compound_Name', 'Adduct', 'Precursor_MZ',\n",
    "                           'INCHI', 'SpectrumID', 'LibraryQualityString',\n",
    "                           '#Scan#', 'MZErrorPPM', 'SharedPeaks']))\n",
    "    ids[-1]['dataset'] = msv_id\n",
    "    pairs.append(pd.read_csv(\n",
    "        f'{ftp_prefix}/PAIRS/{msv_id}_pairs.tsv', sep='\\t',\n",
    "        usecols=['CLUSTERID1', 'CLUSTERID2', 'DeltaMZ', 'Cosine']))\n",
    "    pairs[-1]['dataset'] = msv_id\n",
    "    summary.append(pd.read_csv(\n",
    "        f'{ftp_prefix}/CLUSTERSUMMARY/{msv_id}_summary.tsv', sep='\\t',\n",
    "        usecols=['cluster index', 'sum(precursor intensity)', 'ScanNumber',\n",
    "                 'Original_Path']))\n",
    "    summary[-1]['dataset'] = msv_id\n",
    "    \n",
    "ids = filter_ids(pd.concat(ids, ignore_index=True))\n",
    "pairs = pairs_explain_mass_diff(pd.concat(pairs, ignore_index=True))\n",
    "summary = summary_max_intensity_scan_per_cluster(\n",
    "    pd.concat(summary, ignore_index=True))\n",
    "suspects = combine_suspect_pairs(ids, pairs, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, ax = plt.subplots(figsize=(width * 1.5, height / 1.5))\n",
    "\n",
    "delta_mzs = get_frequent_mass_diffs(pairs['DeltaMZ'])\n",
    "ax.bar(delta_mzs['DeltaMZ'], delta_mzs['count'], width=0.4, color='black')\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "ax.set_xlabel('Delta m/z')\n",
    "ax.set_ylabel(f'Number of pairs')\n",
    "\n",
    "plt.savefig('delta_mz.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mzs.to_csv('delta_mz.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create output for suspect library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output: spectral library batch file\n",
    "\n",
    "#batch upload for adding spectral library\n",
    "(1 spectrum per analog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects.to_csv('suspect_library_5MSV_20200520.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate: \n",
    "\n",
    "Check if molecular formula varies by the same atoms as proposed based on the nominal mass difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in conditionals of what to change / or data to summarize with regards to overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to get from elsewhere: 'PI', 'Data Collector', 'Instrument', 'Ion_Source', 'IonMode' - based on the Unique Filepath"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
