{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of candidate suspect list\n",
    "\n",
    "- Julia M. Gauglitz\n",
    "- Wout Bittremieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftplib\n",
    "import functools\n",
    "import logging\n",
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as ssignal\n",
    "import seaborn as sns\n",
    "import tqdm.notebook as tqdm\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette('Set1')\n",
    "sns.set_context('paper', font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='{asctime} [{levelname}/{processName}] {message}',\n",
    "                    style='{', level=logging.INFO)\n",
    "logging.captureWarnings(True)\n",
    "logger = logging.getLogger('suspect_list')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspect wrangling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_shift_annotations = pd.read_csv(\n",
    "    'https://docs.google.com/spreadsheets/d/'\n",
    "    '1-xh2XpSqdsa4yU-ATpDRxmpZEH6ht982jCCATFOpkyM/'\n",
    "    'export?format=csv&gid=566878567')\n",
    "mass_shift_annotations['mz delta'] = (mass_shift_annotations['mz delta']\n",
    "                                      .astype(np.float64))\n",
    "mass_shift_annotations['priority'] = (mass_shift_annotations['priority']\n",
    "                                      .astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(ids: pd.DataFrame, max_ppm: float = 20,\n",
    "               min_shared_peaks: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter high-quality identifications according to the given maximum ppm\n",
    "    deviation and minimum number of shared peaks. Identifications without an\n",
    "    InChI will be omitted as well.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The tabular identifications retrieved from GNPS.\n",
    "    max_ppm : float\n",
    "        The maximum ppm deviation.\n",
    "    min_shared_peaks : int\n",
    "        The minimum number of shared peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The identifications retained after filtering.\n",
    "    \"\"\"\n",
    "    return (ids[(ids['MZErrorPPM'].abs() <= max_ppm) &\n",
    "                (ids['SharedPeaks'] >= min_shared_peaks)]\n",
    "            .dropna(subset=['INCHI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs: pd.DataFrame, min_cosine: float = 0.8) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only consider pairs with a cosine similarity that exceeds the given\n",
    "    cosine threshold.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs : pd.DataFrame\n",
    "        The tabular pairs retrieved from GNPS.\n",
    "    min_cosine : float\n",
    "        The minimum cosine used to retain high-quality pairs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The pairs filtered by minimum cosine similarity.\n",
    "    \"\"\"\n",
    "    return pairs[pairs['Cosine'] >= min_cosine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(cluster_info: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each cluster select as representative the scan with the highest\n",
    "    precursor intensity.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    cluster_info : pd.DataFrame\n",
    "        The tabular cluster info retrieved from GNPS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Clusters without duplicated spectra by keeping only the scan with the\n",
    "        highest precursor intensity for each cluster.\n",
    "    \"\"\"\n",
    "    cluster_info = (\n",
    "        cluster_info.reindex(cluster_info.groupby(\n",
    "            ['dataset', 'cluster index'])['sum(precursor intensity)'].idxmax())\n",
    "        .dropna().reset_index(drop=True)\n",
    "        [['dataset', 'cluster index', 'parent mass', 'ScanNumber',\n",
    "          'Original_Path']])\n",
    "    cluster_info['cluster index'] = cluster_info['cluster index'].astype(int)\n",
    "    cluster_info['ScanNumber'] = cluster_info['ScanNumber'].astype(int)\n",
    "    return cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suspects(ids: pd.DataFrame, pairs: pd.DataFrame,\n",
    "                      summary: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate suspects from identifications and aligned spectra pairs.\n",
    "    Provenance about the spectra pairs is added from the summary.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ids : pd.DataFrame\n",
    "        The filtered identifications.\n",
    "    pairs : pd.DataFrame\n",
    "        The filtered pairs.\n",
    "    summary : pd.DataFrame\n",
    "        The filtered summary information for the clusters.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with information about both spectra forming a suspect\n",
    "        identification.\n",
    "    \"\"\"\n",
    "    # Form suspects of library and unidentified spectra pairs.\n",
    "    suspects = pd.concat([\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID1'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID1'])\n",
    "        .rename(columns={'CLUSTERID2': 'SuspectIndex'}),\n",
    "        pd.merge(pairs, ids, left_on=['dataset', 'CLUSTERID2'],\n",
    "                 right_on=['dataset', '#Scan#'])\n",
    "        .drop(columns=['CLUSTERID2'])\n",
    "        .rename(columns={'CLUSTERID1': 'SuspectIndex'})],\n",
    "        ignore_index=True, sort=False).dropna(axis=1)\n",
    "    \n",
    "    # TODO: Properly handle this warning.\n",
    "    if not suspects['SuspectIndex'].is_unique:\n",
    "        logger.warning('Multiple analog matches per suspect scan found')\n",
    "    \n",
    "    # Add provenance information for the library and suspect scans.\n",
    "    suspects = (suspects[['dataset', 'INCHI', 'Compound_Name', 'Adduct',\n",
    "                          'Cosine', 'Precursor_MZ', 'SpectrumID', '#Scan#',\n",
    "                          'SuspectIndex']]\n",
    "                .rename(columns={'Compound_Name': 'CompoundName',\n",
    "                                 'Precursor_MZ': 'LibraryPrecursorMZ',\n",
    "                                 'SpectrumID': 'LibraryID',\n",
    "                                 '#Scan#': 'ClusterScanNr'}))\n",
    "    suspects = (pd.merge(suspects, summary,\n",
    "                         left_on=['dataset', 'SuspectIndex'],\n",
    "                         right_on=['dataset', 'cluster index'])\n",
    "                .drop(columns=['SuspectIndex', 'cluster index'])\n",
    "                .rename(columns={'parent mass': 'SuspectPrecursorMZ',\n",
    "                                 'Original_Path': 'SuspectPath',\n",
    "                                 'ScanNumber': 'SuspectScanNr'}))\n",
    "    return suspects.drop(columns=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mass_shifts(\n",
    "        suspects: pd.DataFrame, mass_shift_annotations: pd.DataFrame,\n",
    "        min_delta_mz: float = 0.5, interval_width: float = 1.0,\n",
    "        bin_width: float = 0.002, peak_height: float = 10,\n",
    "        max_dist: float = 0.01) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group close mass shifts.\n",
    "    \n",
    "    Mass shifts are binned and the group delta m/z is detected by finding\n",
    "    peaks in the histogram.\n",
    "    \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    suspects : pd.DataFrame\n",
    "        The suspects from which mass shifts are grouped.\n",
    "    mass_shift_annotations : pd.DataFrame\n",
    "        Mass shift explanations.\n",
    "    min_delta_mz : float\n",
    "        The minimum (absolute) delta m/z for suspects to be retained.\n",
    "    interval_width : float\n",
    "        The size of the interval in which mass shifts are binned, centered\n",
    "        around unit masses.\n",
    "    bin_width : float\n",
    "        The bin width used to construct the histogram.\n",
    "    peak_height : float\n",
    "        The minimum height for a peak to be considered as a group.\n",
    "    max_dist : float\n",
    "        The maximum m/z difference that group members can have with the\n",
    "        group's peak.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The suspects with grouped mass shifts.\n",
    "    \"\"\"\n",
    "    suspects['DeltaMZ'] = \\\n",
    "        suspects['SuspectPrecursorMZ'] - suspects['LibraryPrecursorMZ']\n",
    "    # Remove suspects with an insufficient mass shift.\n",
    "    suspects = suspects[suspects['DeltaMZ'].abs() > min_delta_mz].copy()\n",
    "    # Assign putative identifications to the mass shifts.\n",
    "    for mz in np.arange(math.floor(suspects['DeltaMZ'].min()),\n",
    "                        math.ceil(suspects['DeltaMZ'].max() + interval_width),\n",
    "                        interval_width):\n",
    "        suspects_interval = suspects[suspects['DeltaMZ'].between(\n",
    "            mz - interval_width / 2, mz + interval_width / 2)]\n",
    "        if len(suspects_interval) == 0:\n",
    "            continue\n",
    "        # Get peaks for frequent deltas in the histogram.\n",
    "        bins = (np.linspace(mz - interval_width / 2,\n",
    "                            mz + interval_width / 2,\n",
    "                            int(interval_width / bin_width) + 1)\n",
    "                + bin_width / 2)\n",
    "        hist, _ = np.histogram(suspects_interval['DeltaMZ'], bins=bins)\n",
    "        peaks_i, prominences = ssignal.find_peaks(\n",
    "            hist, height=peak_height, distance=max_dist / bin_width,\n",
    "            prominence=(None, None))\n",
    "        if len(peaks_i) == 0:\n",
    "            continue\n",
    "        # Assign deltas to their closest peak.\n",
    "        mask_peaks = np.unique(np.hstack(\n",
    "            [suspects_interval.index[suspects_interval['DeltaMZ']\n",
    "                                     .between(min_mz, max_mz)]\n",
    "             for min_mz, max_mz in zip(bins[prominences['left_bases']],\n",
    "                                       bins[prominences['right_bases']])]))\n",
    "        mz_diffs = np.vstack([\n",
    "            np.abs(suspects.loc[mask_peaks, 'DeltaMZ'] - peak)\n",
    "            for peak in bins[peaks_i]])\n",
    "        # Also make sure that delta assignments don't exceed the maximum\n",
    "        # distance.\n",
    "        mask_mz_diffs = mz_diffs.min(axis=0) < max_dist\n",
    "        mz_diffs = mz_diffs[:, mask_mz_diffs]\n",
    "        mask_peaks = mask_peaks[mask_mz_diffs]\n",
    "        peak_assignments = mz_diffs.argmin(axis=0)\n",
    "        # Assign putative explanations to the grouped mass shifts.\n",
    "        for delta_mz, peak_i in zip(bins[peaks_i], range(len(peaks_i))):\n",
    "            mask_delta_mz = mask_peaks[peak_assignments == peak_i]\n",
    "            suspects.loc[mask_delta_mz, 'GroupDeltaMZ'] = delta_mz\n",
    "            putative_id = mass_shift_annotations[\n",
    "                (mass_shift_annotations['mz delta'].abs()\n",
    "                 - abs(delta_mz)).abs() < max_dist / 2]\n",
    "            putative_id = putative_id.sort_values(\n",
    "                ['priority', 'atomic difference', 'rationale'])\n",
    "            if len(putative_id) == 0:\n",
    "                suspects.loc[mask_delta_mz, 'AtomicDifference'] = 'unknown'\n",
    "                suspects.loc[mask_delta_mz, 'Rationale'] = 'unspecified'\n",
    "            else:\n",
    "                # TODO: priority.\n",
    "                suspects.loc[mask_delta_mz, 'AtomicDifference'] = '|'.join(\n",
    "                    putative_id['atomic difference'].fillna('unspecified'))\n",
    "                suspects.loc[mask_delta_mz, 'Rationale'] = '|'.join(\n",
    "                    putative_id['rationale'].fillna('unspecified'))\n",
    "    # Set delta m/z's for ungrouped suspects.\n",
    "    suspects['GroupDeltaMZ'].fillna(suspects['DeltaMZ'], inplace=True)\n",
    "    \n",
    "    return (suspects.sort_values(['CompoundName', 'GroupDeltaMZ'])\n",
    "            .reset_index(drop=True)\n",
    "            [['INCHI', 'CompoundName', 'Adduct', 'DeltaMZ', 'GroupDeltaMZ',\n",
    "              'AtomicDifference', 'Rationale', 'Cosine',\n",
    "              'LibraryPrecursorMZ', 'LibraryID', 'ClusterScanNr',\n",
    "              'SuspectPrecursorMZ', 'SuspectScanNr', 'SuspectPath']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate suspect library entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria to form a suspect:\n",
    "\n",
    "- Identification ≤ 20 ppm.\n",
    "- Identification ≥ 6 shared peaks.\n",
    "- Identification has to include InChI.\n",
    "- Cosine ≥ 0.8.\n",
    "- The spectrum with maximal precursor intensity is chosen as cluster representative.\n",
    "\n",
    "Criteria to assign group delta m/z's:\n",
    "\n",
    "- Only delta _m_/_z_'s that exceed 0.5 Da are considered.\n",
    "- Delta _m_/_z_'s are examined within each 1 _m_/_z_ window separately (centered around unit _m_/_z_'s).\n",
    "- Delta _m_/_z_'s are binned with 0.002 bin width.\n",
    "- Peaks with minimum height 10 are extracted from the delta _m_/_z_ histograms.\n",
    "- Delta _m_/_z_'s between the left and right bases of each peak and at maximum 0.01 Da distance from the peak _m_/_z_'s are grouped.\n",
    "\n",
    "Suspect filtering:\n",
    "\n",
    "- Suspects whose delta _m_/_z_ occurs less than 10 times are discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ppm = 20\n",
    "min_shared_peaks = 6\n",
    "min_cosine = 0.8\n",
    "min_delta_mz = 0.5\n",
    "interval_width = 1.0\n",
    "bin_width = 0.002\n",
    "peak_height = 10\n",
    "max_dist = 0.01\n",
    "min_group_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'MSV000084314/updates/2020-10-08_mwang87_d7c866dd/other'\n",
    "ftp_prefix = f'ftp://massive.ucsd.edu/{base_url}'\n",
    "\n",
    "# Get the MassIVE IDs for all datasets processed in the living data analyses.\n",
    "ftp = ftplib.FTP('massive.ucsd.edu')\n",
    "ftp.login()\n",
    "ftp.cwd(f'{base_url}/CLUSTERINFO')\n",
    "msv_ids = [filename[:filename.find('_')] for filename in ftp.nlst()]\n",
    "\n",
    "# Generate the suspects.\n",
    "ids, pairs, clusters = [], [], []\n",
    "logger.info('Retrieve cluster information')\n",
    "for msv_id in tqdm.tqdm(msv_ids, desc='Datasets processed', unit='dataset'):\n",
    "    max_tries = 5\n",
    "    while max_tries > 0:\n",
    "        try:\n",
    "            ids.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/IDENTIFICATIONS/{msv_id}_identifications.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'Compound_Name', 'Adduct', 'Precursor_MZ', 'INCHI',\n",
    "                    'SpectrumID', 'LibraryQualityString', '#Scan#',\n",
    "                    'MZErrorPPM', 'SharedPeaks']))\n",
    "            ids[-1]['dataset'] = msv_id\n",
    "            pairs.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/PAIRS/{msv_id}_pairs.tsv', sep='\\t',\n",
    "                usecols=['CLUSTERID1', 'CLUSTERID2', 'Cosine']))\n",
    "            pairs[-1]['dataset'] = msv_id\n",
    "            clusters.append(pd.read_csv(\n",
    "                f'{ftp_prefix}/CLUSTERINFO/{msv_id}_clustering.tsv',\n",
    "                sep='\\t', usecols=[\n",
    "                    'cluster index', 'sum(precursor intensity)',\n",
    "                    'parent mass', 'Original_Path', 'ScanNumber']))\n",
    "            clusters[-1]['dataset'] = msv_id\n",
    "        except ValueError:\n",
    "            logger.warning(\"Couldn't process dataset %s\", msv_id)\n",
    "            max_tries = 0\n",
    "        except IOError:\n",
    "            max_tries -= 1\n",
    "        else:\n",
    "            max_tries = 0\n",
    "    \n",
    "# Collect and wrangle the data from GNPS.\n",
    "logger.info('Compile suspect pairs')\n",
    "ids = filter_ids(pd.concat(ids, ignore_index=True), max_ppm, min_shared_peaks)\n",
    "pairs = filter_pairs(pd.concat(pairs, ignore_index=True), min_cosine)\n",
    "clusters = filter_clusters(pd.concat(clusters, ignore_index=True))\n",
    "suspects_unfiltered = generate_suspects(ids, pairs, clusters)\n",
    "# Group and assign suspects by observed delta m/z.\n",
    "logger.info('Assign putative explanations to mass shifts')\n",
    "suspects = group_mass_shifts(suspects_unfiltered, mass_shift_annotations,\n",
    "                             min_delta_mz, interval_width, bin_width,\n",
    "                             peak_height, max_dist)\n",
    "# Only use the top suspect (by cosine score) per combination of library\n",
    "# spectrum and putative identification.\n",
    "suspects_unique = (suspects.sort_values(['Cosine'], ascending=False)\n",
    "                   .drop_duplicates(['CompoundName', 'GroupDeltaMZ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mzs = (suspects['GroupDeltaMZ'].value_counts().reset_index()\n",
    "             .rename(columns={'GroupDeltaMZ': 'Count', 'index': 'GroupDeltaMZ'})\n",
    "             .sort_values('Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_unique_filtered = (suspects_unique[\n",
    "    suspects_unique['GroupDeltaMZ'].isin(\n",
    "        delta_mzs.loc[delta_mzs['Count'] >= min_group_size, 'GroupDeltaMZ'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total: {len(suspects):,} suspects collected')\n",
    "print(f'After duplicate removal and filtering '\n",
    "      f'(delta m/z occurs at least {min_group_size} times): '\n",
    "      f'{len(suspects_unique_filtered):,} unique suspects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspects_unfiltered.to_csv('../../data/suspects_unfiltered.csv', index=False)\n",
    "suspects_unique_filtered.to_csv('../../data/suspects_unique.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
